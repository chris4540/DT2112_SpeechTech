\documentclass[12pt]{article}

% packages
\usepackage[margin=1in]{geometry}
\usepackage{enumerate}
\usepackage[numbers]{natbib}
\usepackage[hidelinks]{hyperref}
\usepackage{csvsimple}

% \setlength{\parindent}{1em}
\setlength{\parskip}{.5em}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\def\printemptyline#1{\def\par{\ifvmode\emptyline\fi\endgraf}\obeylines}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\begin{document}

\title{DT2112 home exam spring 2019}
\author{Lin Chun Hung, chlin3@kth.se}
\maketitle

\section{Automatic speech recognition and human perception}
\begin{problem}{1.1}
    A speech parametrization is for extracting the feature vectors or
    the speech parameter vectors from speech signal. In the speech parametrization,
    the mel-frequency cepstrum coefficients (MFCCs) are most likely used for
    representing a short term power spectrum of speech. The size of the
    feature vectors represented by MFC method is 39 typically.

    In HMM-based ASR, the speech parameter vectors is a series of observations
    for training the HMM models. Each HMM model corresponds to one word.

    The acoustic model is a representation of the relationship between feature
    vectors (observations) and subphonemes (hidden states).
    Typically, each phoneme has three subphonemes. The Hidden Markov Model with
    Gaussian Mixture Models are used as the acoustic model in HMM-based ASR.

    The HMMs for different words are trained and we compare the model likelihoods
    by forward-backward algorithm for isolated words recognition. For continuous
    speech recognition, we join our word models with the silent state and use viterbi
    algorithm to decode words from speech.

    The language model is to provide the information about the priori probability
    of the word sequence $P(word)$. A language model is able to disambiguate between
    similar acoustics when combining linguistic knowledge and acoustic evidence.
    % For example, "ice cream" and "I scream" can be disambiguated with the language
    % model even though their pronunciation are the same.

    In HMM-based ASR, n-gram model is used as the language model and it is trained
    on millions of words of text.

    % an N-best list
    The N-Best list is a list contains N ranked possible decoding for the speech
    from the user.

    The reason why developers would like to get an N-best list as an ASR result
    is the top hypothesis may be erroneous and may not fit the content of a
    dialogue system and the correct hypothesis is often in the N-best list.
    Therefore, the N-best list can increase the robusness of a dialogue system.
\end{problem}


\begin{problem}{1.2}
    For the speech parametrization, MFCC is based on human hearing perceptions or
    in other words is to mimic known variation of the critical bands of
    human ears to different frequencies. Human ears are more discriminative at
    lower pitch and less discriminative at higher pitch and therefore the mel
    filterbank varies linearly below 1kHz and logarithmically above 1kHz.

    For the acoustic model, the hidden markov model considers the transition
    probability of a phoneme (state) to a phoneme or staying on one phoneme.
    It is in the same way as a proficient language user that understanding
    what a particular speech should sound like in relation to adjoining sounds.
    Humans would probabay understand a speech as a series of phonemes instead of
    phonemes independently.

    The language models gives the priori probability of a certain word in a
    particular context given the adjoining words. It is similar to how human
    guess a homophone word since human can guess the word by the converstation
    content and the regularity of the word based on the knowledge and understanding
    of a certain language.

    Since human and machine make error, the best guess of a word or a sentence may
    be erroneous. Human would perpare a list of guess of a speech and choose the
    best fit word or sentence given the converstation content and the intention
    of the speaker. The n-best lists can be considered as an imitation of this behavior.
\end{problem}

\begin{problem}{1.3}
    % https://www.youtube.com/watch?v=p5RWzBRg6rU
    % 1. The pitch of the speech
    % * signify questions
    % * emphasize the main point of the speech
    The first way can affect our perception is the pitch of the speech. In English,
    the pitch can indicate whether this word is a verb or a noun. The pitch
    can indicate the main point of a sentence and signify it is a question. 
    For example,
    "You stole the book" with a high pitch at the end is a kind of question 
    expecting an answer. Humans can easily understand this is a question but 
    an automatic speech recognition (ASR) system would only translate it to 
    a sentence if no grammatical cues. 
    
    % http://www.speech.kth.se/~rolf/gslt_papers/MustaphaSkiri.pdf
    % 2. The visual event.
    % * Eye gazing refer to physical thing and fit to the speech context.
    % * McGurk effect 
    The second way is the visual event or the visual cue. Our eys gaze is a 
    strong indicator and it can give information to the speech context. A simple
    example is that one speaker say "Close the door please." and then eyes on that
    door would give the location information of "the door". Another example is 
    a girl say "The room is very cold." and she gazes at the windows. Human listener
    can understand that the girl is asking if the listener can close the windows.
    Humans can understand these visual cues but a ASR would not understand the
    context from the visual cues and just transcribe the speech to text.

    The another example for the visual event affecting speech perception 
    is the McGurk effect. This effect can be found when the speech sound does 
    not match to the lip movement. For example, the audio is "ba" but the lip 
    movement is "fa" and the McGurk effect takes place. Humans understand "fa"
    sound in the last example but a ASR system understand it as "ba" sound since
    the system just able to receive sounds.
\end{problem}

\pagebreak

\begin{problem}{2.1}
    In formant synthesis, speech output is parameterized by formant frequencies,
    fundamental frequency, their amplitudes, and noise levels etc. as a function
    of time. Formant speech synthesis is based on rules which describe
    the resonant frequencies of the vocal tract.

    In unit selection synthesis, the output speech is synthesized by selecting
    appropriate speech units from large databases of recorded speech.
    The speech units can be phone, diphone, half-phones or words.

    In diphone synthesis, speech is generated from a database which contains
    every diphone within the language.
    Rather using a large database including all speech units in unit selection
    synthesis, diphone synthesis only uses diphone as its basic unit.

    In HMM-based synthesis, the frequency spectrum,
    fundamental frequency (F0), and duration of speech
    are modeled simultaneously by HMMs.
    Speech are created based on the maximum likelihood estimation of state transitions.

    HMM-based synthesis is not concatenating different recordings
    as unit selection synthesis and diphone synthesis but generating speech parameters
    from the HMM models. And then a speech is synthesized from the generated parameters.

    During the systhesis phrase, unit selection synthesis is serial while HMM-based
    systhesis is parallel.
\end{problem}

\begin{problem}{2.2}

    % co-articulation yes, as in diphone
    Co-articulation means speech sounds tend to be influenced by other speech
    sounds surronding them.
    Diphone synthesis is able to capture this transition from one phonic unit
    to another unit.
    Therefore, unit selection synthesis is able to do the same thing that as
    diphone synthesis.

    The co-articulation is perfectly retained in the diphone or larger
    units.
    Therefore the unit selection synthesis can completely reproduce
    the co-articulation in synthesized speechs.


    % speech reduction
    Speech reduction means a speaker skips or changes some syllables when talking
    at normal speed. Unit selection synthesis can do the speech reduction at
    some degree and it depends on the speech data in database and how to annotate
    the text with phonetic units.
    For example, "Jag vet inte" in written Swedish is spoken as "javende". If
    the database have this phrase, then the synthesis can correct pronounce it
    correctly. On the other hand, if the database does not have this phrase,
    we have to annotate phonetic units correctly. However, the way to annotate
    phonetic units is case-by-case and it also depends on the context of the
    conversation.

    For human speakers, it is a natual action for native speakers and it depends
    on the knowledge to that language of the speakers.

    % hesitation
    For classical speech systhesis, it is hard to find any hesitations like
    a pause or an "ehm" since it is from text to speech.
    If we want to add hesitations in unit
    selection synthesis, we can add an extra module to analyze which part of the
    sentence would easily have a pause or an "emmmm" and then add hesitations with
    a low frequency.

    To compare with human speakers, the hesitations in the speech systhesis system
    may not be very natural since the analysis module can go wrong. Another point
    is hesitations is closely related to the conversation context and therefore
    human listeners may not feel natual to the hesitation.
    % using a random frequency to add a pause may not be very natual to humans.

    % word repetition
    In speech systhesis, word repetition would not happen since the systhesis
    system "translates" text to speech programmatically. Also, the speech data in
    the database is from a group of professional speakers and the data has quality control.
    Therefore it is hard to find word repetition in unit selection synthesis.

    To add word repetition, we can add a module to consider where should have
    word repetition such as the main topic of the sentence or the difficult words
    in the sentence and then create word repetition in the systhesised speech according
    to the analysis result.

    In unit selection synthesis, the word repetition is expected as a rare event
    since the synthesized speech sounds like from a professional speaker and therefore
    the word repetition phenomenon should not be very frequent.
\end{problem}

\begin{problem}{2.3}
    % The system should stop when received the voice commands like pause, stop,
    % go to the next section, and go back to last section. Also, at the end of a 
    % section and the end of the text book we should have a few seconds pause to 
    % indicate that this is the end of a section. 
    When the system is going to stop,
    the system should say a few more syllables or the unit for the system. It 
    makes the system more human-like. The system can be suddenly stopped but it 
    will be very unnatural to human listener. The system can stop at the end of 
    the processing sentence. However, some sentences would be very long and the 
    system cannot within a short period of time.

    When resuming speaking, go back to the begining of the sentence is a good 
    choice since it gives the sense of integrity of the speech. Resuming at the 
    point we stop is a little bit odd and strange to human. We can think that 
    a human read out the speech would not simply starting from the word he/she stop.
    A few word before the pause is sensible since most of the people will do it 
    in the same way.

    The first suggestion is to consider the syntheized speech as an audio file.
    We mark the information like the time tag of the sentence start 
    and the tags of few words for every syntheized speech. When we stop and resume
    the reading system, the software just need to search the nearest time tag 
    before the pasue point and then resume it in the same way.
    This method is good because it is easy to understand and implement it.
    The pause is not quite human-like because we will not pasue it like a playing
    machine.

    The second suggestion is online re-synthesize the ending pharse 
    (the few words after receiving the pause command) and the 
    starting words (the few words before the puase point). The tone and the prosody
    will be adjusted in this situation and therefore it is more human-like.

    The third suggestion needs drop a sentence into segments. 
    We first use a model to sperate a sentence into segments and each segment has a 
    resonable number of words, say like at most 10 words. 
    The model can be a statistical model or a rule-based model. 
    When the speech synthesis system create a speech, one speech is for a segment
    but not for the whole sentence. This is to emulate the human understanding 
    structure of a sentence. When we read a long sentence, we usually sperate it
    into a few parts. The speration may be from conjunction words, punctuation marks,
    a clause, or the meaning of a group of words. When we need to stop, we stop
    at the end or current segment speech and restart at the next segment speech.

\end{problem}

\pagebreak
% Question 3
\begin{problem}{3.1}
    % smart home manager
    % data checking
    % https://peterhurley.com/blog/2018/google-assistant-vs-human-assistant-featuring-peter-hurley
    I would like to use Google Home Assistant to mention 4 ways where a dialogue
    system perform a well-defined dialogue task differs from a human to do so.

    % Physically
    First, a human assistant barely replaces you in the physical interation or 
    operation but it is different to Google Home Assistant.
    For example, I call Peter, my human assistant, to turn on the light of the 
    living room and then he goes to the switch of the light and turn it on. On 
    the other hand, the Google Home Assistant can help you turn on the light
    through the home intranet without any physical operation. 

    % Information Obtaining time 
    Second, the time for obtaining information is different. A spoken dialogue 
    system will be much faster. For example, I ask Google Home Assistant what
    is the tempertrue today and it will response me just one second. A human 
    assistant needs time to go to check the online website. For another question
    like "Any milk in the fridge?", the same thing happens.

    % Visual interation
    However, a human assistant performs better when the task needs visual
    cues, visual interactions, my position information, head pose, and/or eye
    gaze. For example, I say "Close the door please." and the human assistant
    could understand that I want to close the door in the room I am currently
    staying in. However, the Google assistant may ask "Which door?" or say
    "I don't understand your order. Could you say one more time?".

    % Robustness with understanding error.
    Lastly, a human assistant have a high robustness with understanding error.
    When I say "Prepare the oven please" which means I would like to preheat the 
    oven, the Google assistant may be not understand my order as the system does
    not expect it, whereas a human assistant is able to understand my order and 
    do it correctly.
\end{problem}

\begin{problem}{3.2}
    % https://botsociety.io/blog/2018/03/chatbot-examples/
    For the application fit the "interface replacement" view, an application called 
    1800Flowers which allows users to order flowers and the delivery service.
    It is because a web form or a online flower shop website can have 
    the same functionality as this application.

    % https://www.facebook.com/DrCareAi/photos/a.2123527274339622.1073741828.2076696632356020/2316047921754222/?type=3
    For the application fit the "human substitute" view, a chatbot with speech 
    support called "Dr Care helper" from Hong Kong is for users to make an 
    appointment with a medical
    specialist, give medical advices, check how full the public hospital is, and 
    estimate how long a patient have to wait for a specialist in the public hospital.
    (Yes, Hong Kong has a serious medical service shortage in public sector.)
    This chatbot gathers all the related information and give some meaningful
    suggestions to users. Therefore, the chatbot can be considered to be like a
    human helper.

    % https://sumo.com/stories/ecommerce-chatbot-marketing
    % https://www.chatbotguide.org/ebay
    Some spoken dialogue systems are less obvious to see which group they belongs
    to. For example, it is hard to classify the shopbot from ebay and Google
    Assistant.
    
    The shopbot can help you to find an item with a vage request like 
    "I am looking for a jacket" and then suggest some items. Also, you can buy
    the selected item and send it to your home. Therefore, it is hard to classify 
    because the application combines the features of the chatbot from these 
    two categories.

    The good thing for the shopbot is it increases the user experience. From 
    introducting product to make payment, you can complete all the task within 
    one chatbot. It makes the chatbot like a salesman. The disadvantage of this
    chatbot is its complexity. When the number of avaibable tasks increases, 
    the implementation difficulty and the error rate increases. When the chatbot
    recognized a speech wrong, it will easily get a wrong conversation intention
    since the number of intention is high for a complex chatbot.

    % pros and cons for google assistant
    Google assistant on smart phones are be considered as a human substitute (e.g,
    play games via speech, book a table, or sing the happy birthday song)
    and a interface replacement (e.g.
    set an alarm or a timer via speech, search something via speech, and set a 
    reminder). The pros is
    the google assistant is more human-like and as a true "virtual assistant" since
    the assistant integrated with other Google services. The cons is that 
    user privacy are not really respected.
    Your travel schedule, photos, and even your voice would be sent to 
    Google for their analysis.
\end{problem}

\begin{problem}{3.3}
    The Google assistant on my phone is used as the subject for this task.
    Japanese is chosen as the language for the investigation.

    \textbf{Describe the system}

    The Google assistant has at least 4 components: automatic speech recognizer (ASR),
    dialog manager, response generation, 
    and text-to-speech (TTS) synthesis. First, the ASR transforms the vioce input 
    from the user to text. Second, the dialog manager interprest the intent
    of the recognized speech and then obtain the relevant information from 
    other modules. And then, response generation components create a text as 
    a response and finally the text-to-speech synthesis component will read it
    out.

    I asked three kinds of questions, the current weather, 
    my current location, and
    what can you (refer to the assistant) do. Each question has two manners:
    normally (teineigo, polite language) and respectfully (sonkeigo, respect language).
    Since the assistant shows what it listened and what it will say, the TTS
    synthesis component and the ASR component show that they exists. With a 
    slightly variation of the questions, they can do what I expected like 
    tell me back the tempertrue, a brief weather report (raining, snowing or cloudy)
    tell me back what Stockholm district I am in and the brief introduction of 
    its functionality.

    \textbf{Describe the interaction}

    The dialogue is like a ping-pong game, the assistant almost response immediately.
    There is no restriction of the topics and I can ask whatever I want.
    I need to press the "microphone" buttom to say something and the response 
    from the assistant cannot stop. Therefore, user cannot interrput the conversation
    or the system. The system show what it was recognized on the screen. For the
    confirmation, the assistant will ask "<comfirmation context>, is it okay?" to
    seek my confirmation.

    \textbf{Describe the error management}

    When I say something wrong, there are two situations. The first one is they
    have a matched intent and the assistant will bring the conversation to that 
    intent. The second one is the assistant said that it does not understand
    what you are saying and ask me say one more time. It is easy to correct
    misunderstandings and I just need to say one more time. If I have multiple
    errors, I would need to start over the conversation again. For example, 
    I asked the assistant to set an alarm and I failed to let the assistant
    understand the time I said. The assistant will drop the current conversation
    and I need to do it again.

    \textbf{Discuss the user friendliness the system}

    The system is easy to use. I can say whatever I think and it allows a great
    variant of my questions. The system is efficient and I do not need to wait
    for its response. I like to use it for its multi-functional and it is a 
    good replacement for the phone user interface. I do not need to press on 
    the screen a lot in order to do some simple tasks.
\end{problem}

\pagebreak
% Question 4
\begin{problem}{4.1}
    Steps to make a data collection
    \begin{enumerate}
        \item Consider the purpose of the collected data. The purpose may be
        studying some linguistic behavior or making a speech recognition system.
        \item Design or determine data collection environment, equipments and technology.
        E.g. slient lab, telephone, or meeting room environment.
        What equipments would be used? What audio format should be used?
        \item Participant recruitment.
        \item Collect data of participant: name, gender, age, education, country born or raised in.
        \item Select participants for background varaible control and/or ecological validity
        \item Determine the best date and time for collecting data.
        \item Detemine in what form to collect data.
        E.g. reading text or an emulated situation with given topics.
        \item Data analyze, quality checking, and/or audio data to text transcription
    \end{enumerate}
\end{problem}

\begin{problem}{4.2}
    % Switchboard: https://catalog.ldc.upenn.edu/LDC97S62
    % Columbia Games Corpus: http://www.cs.columbia.edu/speech/games-corpus/
    % D64: http://www.uni-bielefeld.de/lili/personen/pwagner/papers/lrec10_oertel_etal.pdf
    \begin{table}
        \centering
        \csvautotabular[respect dollar=false]{csv/task_4_2_p1.csv}
        \caption{Basic properties of the corpora}
        \label{tbl:Q42_corpora_properties}    
    \end{table}

    \begin{table}
        \centering
        \csvautotabular[respect dollar=false]{csv/task_4_2_p2.csv}
        \caption{Characteristic of the corpora}
        \label{tbl:Q42_corpora_char}    
    \end{table}

    The basic properties of the corpora can be found in the Table \ref{tbl:Q42_corpora_properties}.

    The Characteristic of the corpora can be found in the Table \ref{tbl:Q42_corpora_char}.

\noindent\textbf{Purpose}

    The Switchboard is a general purpose corpus. It is suitable for building,
    testing, and evaluating an automatic speech recognition system targeting to 
    telephone conversations. \cite{switchboard_intro}

    The Columbia Games Corpus is originally for a study of the intonational 
    realization of given verse new information. 
    It can also be used in developing and evaluating an interactive voice 
    response (IVR) systems especially in the aspect of turn generation and recognition.
    \cite{moller2007evaluating}

    The D64 corpus is for studying naturalistic, ethologically situated, 
    and conversational interaction \cite{oertel2013d64}. Also, 
    the motivation of building this corpus was to make the participants to focus
    on social interaction. It could reflect on posture, hand and arm gestures, 
    and eye gaze \cite{oertel2013d64}.

\noindent\textbf{The highlighed characteristic}
    
    The most different characteristic of the switchboard corpus is its variety
    and rich annotations. The variety and the richness of annotations can make
    the evaluatation of a speech recognition system more representative.

    The highlighed characteristic of the Columbia Games Corpus is its co-operation
    task-oriented dialogue. The speechs invloves coordination of speakers and 
    turn exchanges between them. In evaluating a speech recognition system for 
    multiple users, this corpus may be helpful.

    For D64 corpus, the rich recording will be the highlighed point. It would be
    helpful for those visual speech systems.

\end{problem}

\begin{problem}{4.3}
    % sub-question 1
    For a corpus for training acoustic models for speech recognition of street names
    in an in-car environment, three features are important. They are
    control over the environment, age balance, and gender balance.

    In a car cabin, it could be quite in as the car is new or noisy as it has a 
    old engine.
    Opening a car window will greatly raise the noise level of the cabin. Therefore,
    we should have a good control over the environment and select multiple noise
    levels for emulating the real-world situation. 
    The driver voices would have a great variety since anybody over 18 years old
    could obtain a driving license.
    Therefore the age and gender of speakers should be balanced.

    % sub-question 2
    For a corpus for broad vocabulary unit selection synthesis of academic literature,
    audio quality, control over the task performed, and control over the linguistic 
    background of subjects are the most important features.

    Ideally more professional academic materials can lower the target cost and 
    the join cost of the system. Therefore, the control over the task performed
    is important. The recording quality and the background of the speaker is 
    important since it is for a speech synthesis system.
   
    % sub-question 3
    For a corpus of a spoken dialogue system which guides vistors to a museum 
    for studying semantic concepts, 
    % the body and audio responses and of visitors could be important.
    control over the taskes in the dialogue, recordings of a large number of subjects,
    and the mobility of the subjects are important.

    In that dialogue system, pointing directions or other instructions need to be well
    designed and therefore the control over the taskes is important. Since it is
    a guiding dialogue system, the subjects should be able to move around.
    Semantic concepts should be converage when we have a large population and 
    therefore we need a large number of subjects.    

    % sub-question 4
    A corpus for scientific studies of breath, posture, and speech for turn-taking
    in human face-to-face interaction should focus on if it is an ecologically 
    valid task, synchronization of different recordings and 
    extensive recordings of each subject.

    Since the corpus is for studying human interactions, a normal circumstance
    would be a requirement. A laboratory environment may not be a good idea.
    Also, the visual speech relation is important in this task and therefore
    the time alignment between video recordings and audio recordings is important. 
    To capture the detailed interactions in a dialogue, extensive recordings are
    necessary for breath, heart beat rate etc.
\end{problem}
\pagebreak

% Question 5
\begin{problem}{5.1}
    Important steps of the process of evaluation:
    \begin{enumerate}
        \item Determine the target or purpose of the evaluation. This affects what kind
        of tests we need to do. For example, if we want to compare two
        speech synthesis systems, the grading test and the preference test will
        be helpful.

        \item User studies for representativeness. This is for controling the
        background varaibles in order to have a more representative evaluation
        result.

        \item Identify, select, and implement evaluation methods.
        It is mainly based on our purpose of the evaluation. For example, if the
        purpose of evaluation is to compare the new system and the existing system,
        grading tests and preference tests should be selected as the evaluation
        tests. For the implementation, it is down to details like design the questionaire
        for the grading tests in order to have a good construct validity.

        \item  Analyze and interpret results.
        There are multiple tasks related to this stage. Take a few tasks as examples.
        Calculating scores in the tests indicates the effectiveness of the speech system.
        Summerizing the test results and identifying the reason why A system is
        better than B system are the tasks should be done in this phrase. Moreover,
        the third party spectator can review the videos of the test period and it
        gives another point of view of interpreting the evaluation results.

    \end{enumerate}
\end{problem}

\begin{problem}{5.2}
    Speech Recognition system and speech Synthesis system are chosen for this
    discussion.

    For a research instiution, the target of evaluation is to test hypotheses.

    In speech recognition, "good" means a modification perform better at some situation.
    For example, they change the feature extraction scheme from MFCCs to a new method
    and test if the accuracy of the new method is better than the MFCCs method.
    The data they collect is the number of words in the test utterance and
    the number words correctly recognized.

    In speech systhesis, "good" can mean how user understand the synthesized speech
    of a new speech systhesis system.
    If the user can understand the context given by the synthesized speech, the
    new speech systhesis system can be considered as good.
    The data like the score of a listening comprehension test should be collected
    for this purpose.

    To a R\&D company selling the underlying technology, the evaluation
    target could be how the new technology better than the existing one. The 
    evaluation can aslo be done on the current version and the old version of 
    the application in order to see the progress of development.

    In speech recognition, the company should compare accuracy across different 
    recognition technologies. Moreover, the performance and the stability of different 
    speech recognition technologies should be evaluated. The data like process time,
    accuracy and performance across a population of speakers should be collected.

    In speech systhesis, the data like intelligibility, smoothness, speed, and 
    fluency of the syntheized speech to human listeners should be collected. 
    Also, the performace and the computational resources like storage, computational
    power needed for each technology should be evaluated.
    The comparsion between two or more technologies on these asspects should be 
    made and see how good the new technology is.

    To a company selling the service to end users, the evaluation targets are
    the completeness of the service and the cost of the services.
    The evaluation for the completeness of the service is similar to the acceptance
    testing. The evaluation of cost services can be obtained by checking hardware
    cost, maintenance cost etc.

    In speech recognition, the data like a list of expected functionalities that 
    the application is able to acheive should be collected. In speech systhesis,
    the completeness of expected functionalities acheived
    should be collected. For example, we should check if the application is able
    to produce a minimal pairs that skilled listeners can distinguish.

    To an end user, the evaluation could be on the user experience. 
    The user experience questionaire should be used and then ask some general
    questions since the user experience should be user-oriented and we cannot
    ask very detailed questions. If the general feedback of the application is 
    positive comparatively, then the application can be considered as good.

    The questionaire should be designed very carefully and the different applications
    should have a slightly different sets of questions. 
    For speech recognition, one of the example questions would be "Do you satisfy
    with the speech recognition for completing your task?"
    For speech systhesis, it would be "do you understand the context or meaning
    of the synthesized speech?" 
    Moreover, third party spectators would be needed in order to have a more objective
    evaluation results for the user experience.
\end{problem}

\begin{problem}{5.3}
    Three academic research evaluations are selected. 
    
\noindent\textbf{Motivations}

    The first evaluation method is for evaluating the performance of a speech recognition 
    method in a noisy condition. \cite{hirsch2000aurora} and 
    the second and the third evaluation method are for a HMM-based speech synthesis system 
    in Thai language \cite{chomphan2007implementation}. 
    
    The second evaluation method is to present the correctness of the synthesized
    tone since Thai language is a tonal language and has five tones.
    
    The third method is to evaluate the syntheized speech quality.

\noindent\textbf{Methods}

    In the first evaluation method, they first prepared a speech data database 
    for training and testing the speech recognition system.
    The speechs in the database was first obtained from 
    the corpus called TIdigits and then add 8 different real-world noises to 
    the speech with different noise ratios \cite{hirsch2000aurora}.
    After training the speech recognition system, the system tested with different
    conditions test set and evaluate the performace based on word accuracy.

    In the second method, they asked listeners to evaluate the correctness of the 
    tone in a synthesized speechs syllable-by-syllable. \cite{tone_questions} 
    
    In the third task, they asked listeners to give the mean opinion score to 
    the naturalness of synthesized speechs. After that, they asked listeners
    to compare the naturalness of synthesized speechs from the HMM-based system and 
    a unit-selection-based TTS system and then give the relative scores.    

\noindent\textbf{Human judge involved or automatic measures}
    
    The first evaluate method is objective and could be done automatically since
    all test data are from the corpus which means it should be labeled.

    The second and the third evaluation method are subjective and involve human judges.
    I would say that the second evaluation method is less subjective since 
    distinguishing different pitches of speechs should be a common standard to
    native speakers.

\noindent\textbf{Critique}

    In the first evaluation method, using the speech data beyond digits would 
    be more representative since the corpus TIdigits has only speechs of
    connected digit in American English. However, the cost of collecting speechs
    will be high if they cannot find an existing corpus.

    In the second method, a naive algorithm for compare tones should be used 
    in order to have an objective result. For example, in a pair of speechs, 
    one is systhesised and one is natural, align and normalize the syllable 
    and calcuate the  moving average of the F0 frequency of the pair of syllable.
    After that, compare their moving average frequency numerically.

    In the third method, the questionaire that we can find online  mentions 
    that the naturalness should be concerned. Adding more questions for different
    aspects like understandable and speed may help to improve this evaluation.
\end{problem}
\pagebreak

\bibliographystyle{IEEEtranN}
\bibliography{ref}
\end{document}
