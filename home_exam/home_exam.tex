\documentclass[12pt]{article}

% packages
\usepackage[margin=1in]{geometry}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{subfig}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{cleveref}
\usepackage{listings}
\usepackage[makeroom]{cancel}
\usepackage[toc,page]{appendix}
\usepackage{enumerate}
\usepackage{csvsimple}
\usepackage{longtable}
\usepackage{pgfplotstable}
\usepackage{booktabs}

% \setlength{\parindent}{1em}
\setlength{\parskip}{.5em}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
% \newcommand*\descriptionlabel[1]{\hspace\leftmargin$#1$}
\def\printemptyline#1{\def\par{\ifvmode\emptyline\fi\endgraf}\obeylines}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\begin{document}

\title{DT2112 home exam spring 2019}
\author{Lin Chun Hung, chlin3@kth.se}
\maketitle

\section{Automatic speech recognition and human perception}
\begin{problem}{1.1}
    A speech parametrization is for extracting the feature vectors or 
    the speech parameter vectors from speech signal. In the speech parametrization,
    the mel-frequency cepstrum coefficients (MFCCs) are most likely used for
    representing a short term power spectrum of speech. The size of the 
    feature vectors represented by MFC method is 39 typically. 

    In HMM-based ASR, the speech parameter vectors is a series of observations
    for training the HMM models. Each HMM model corresponds to one word.
    
    The acoustic model is a representation of the relationship between feature 
    vectors (observations) and subphonemes (hidden states).
    Typically, each phoneme has three subphonemes. The Hidden Markov Model with 
    Gaussian Mixture Models are used as the acoustic model in HMM-based ASR.

    The HMMs for different words are trained and we compare the model likelihoods
    by forward-backward algorithm for isolated words recognition. For continuous
    speech recognition, we join our word models with the silent state and use viterbi 
    algorithm to decode words from speech.

    The language model is to provide the information about the priori probability
    of the word sequence $P(word)$. A language model is able to disambiguate between
    similar acoustics when combining linguistic knowledge and acoustic evidence.
    % For example, "ice cream" and "I scream" can be disambiguated with the language
    % model even though their pronunciation are the same.
    
    In HMM-based ASR, n-gram model is used as the language model and it is trained
    on millions of words of text.
    
    % an N-best list 
    The N-Best list is a list contains N ranked possible decoding for the speech
    from the user.

    The reason why developers would like to get an N-best list as an ASR result
    is the top hypothesis may be erroneous and may not fit the content of a 
    dialogue system and the correct hypothesis is often in the N-best list. 
    Therefore, the N-best list can increase the robusness of a dialogue system.
\end{problem}


\begin{problem}{1.2}
    For the speech parametrization, MFCC is based on human hearing perceptions or
    in other words is to mimic known variation of the critical bands of 
    human ears to different frequencies. Human ears are more discriminative at 
    lower pitch and less discriminative at higher pitch and therefore the mel
    filterbank varies linearly below 1kHz and logarithmically above 1kHz.
    
    For the acoustic model, the hidden markov model considers the transition
    probability of a phoneme (state) to a phoneme or staying on one phoneme. 
    It is in the same way as a proficient language user that understanding 
    what a particular speech should sound like in relation to adjoining sounds.
    Humans would probabay understand a speech as a series of phonemes instead of
    phonemes independently.

    The language models gives the priori probability of a certain word in a 
    particular context given the adjoining words. It is similar to how human 
    guess a homophone word since human can guess the word by the converstation
    content and the regularity of the word based on the knowledge and understanding
    of a certain language.

    Since human and machine make error, the best guess of a word or a sentence may
    be erroneous. Human would perpare a list of guess of a speech and choose the 
    best fit word or sentence given the converstation content and the intention
    of the speaker. The n-best lists can be considered as an imitation of this behavior. 
\end{problem}

\begin{problem}{1.3}
    TODO
\end{problem}

\begin{problem}{2.1}
    In formant synthesis, speech output is parameterized by formant frequencies,
    fundamental frequency, their amplitudes, and noise levels etc. as a function 
    of time. Formant speech synthesis is based on rules which describe
    the resonant frequencies of the vocal tract.

    In unit selection synthesis, the output speech is synthesized by selecting 
    appropriate speech units from large databases of recorded speech. 
    The speech units can be phone, diphone, half-phones or words.

    In diphone synthesis, speech is generated from a database which contains 
    every diphone within the language. 
    Rather using a large database including all speech units in unit selection
    synthesis, diphone synthesis only uses diphone as its basic unit.

    In HMM-based synthesis, the frequency spectrum, 
    fundamental frequency (F0), and duration of speech 
    are modeled simultaneously by HMMs. 
    Speech are created based on the maximum likelihood estimation of state transitions.

    HMM-based synthesis is not concatenating different recordings 
    as unit selection synthesis and diphone synthesis but generating speech parameters
    from the HMM models. And then a speech is synthesized from the generated parameters.

    During the systhesis phrase, unit selection synthesis is serial while HMM-based
    systhesis is parallel.
\end{problem}

\begin{problem}{2.2}

    % co-articulation yes, as in diphone
    Co-articulation means speech sounds tend to be influenced by other speech 
    sounds surronding them.
    Diphone synthesis is able to capture this transition from one phonic unit 
    to another unit.
    Therefore, unit selection synthesis is able to do the same thing that as 
    diphone synthesis.

    The co-articulation is perfectly retained in the diphone or larger 
    units. 
    Therefore the unit selection synthesis can completely reproduce 
    the co-articulation in synthesized speechs.
    

    % speech reduction
    Speech reduction means a speaker skips or changes some syllables when talking
    at normal speed. Unit selection synthesis can do the speech reduction at 
    some degree and it depends on the speech data in database and how to annotate 
    the text with phonetic units. 
    For example, "Jag vet inte" in written Swedish is spoken as "javende". If
    the database have this phrase, then the synthesis can correct pronounce it
    correctly. On the other hand, if the database does not have this phrase, 
    we have to annotate phonetic units correctly. However, the way to annotate
    phonetic units is case-by-case and it also depends on the context of the 
    conversation.

    For human speakers, it is a natual action for native speakers and it depends
    on the knowledge to that language of the speakers.

    % hesitation
    For classical speech systhesis, it is hard to find any hesitations like 
    a pause or an "ehm" since it is from text to speech. 
    If we want to add hesitations in unit
    selection synthesis, we can add an extra module to analyze which part of the 
    sentence would easily have a pause or an "emmmm" and then add hesitations with
    a low frequency.

    To compare with human speakers, the hesitations in the speech systhesis system
    may not be very natural since the analysis module can go wrong. Another point
    is hesitations is closely related to the conversation context and therefore
    human listeners may not feel natual to the hesitation.
    % using a random frequency to add a pause may not be very natual to humans.

    % word repetition
    In speech systhesis, word repetition would not happen since the systhesis
    system "translates" text to speech programmatically. Also, the speech data in
    the database is from a group of professional speakers and the data has quality control.
    Therefore it is hard to find word repetition in unit selection synthesis.

    To add word repetition, we can add a module to consider where should have 
    word repetition such as the main topic of the sentence or the difficult words
    in the sentence and then create word repetition in the systhesised speech according
    to the analysis result.

    In unit selection synthesis, the word repetition is expected as a rare event
    since the synthesized speech sounds like from a professional speaker and therefore
    the word repetition phenomenon should not be very frequent.
\end{problem}

\end{document}
