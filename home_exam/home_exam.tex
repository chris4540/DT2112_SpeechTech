\documentclass[12pt]{article}

% packages
\usepackage[margin=1in]{geometry}
\usepackage{enumerate}
\usepackage[numbers]{natbib}
\usepackage[hidelinks]{hyperref}



% \setlength{\parindent}{1em}
\setlength{\parskip}{.5em}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
% \newcommand*\descriptionlabel[1]{\hspace\leftmargin$#1$}
\def\printemptyline#1{\def\par{\ifvmode\emptyline\fi\endgraf}\obeylines}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\begin{document}

\title{DT2112 home exam spring 2019}
\author{Lin Chun Hung, chlin3@kth.se}
\maketitle

\section{Automatic speech recognition and human perception}
\begin{problem}{1.1}
    A speech parametrization is for extracting the feature vectors or
    the speech parameter vectors from speech signal. In the speech parametrization,
    the mel-frequency cepstrum coefficients (MFCCs) are most likely used for
    representing a short term power spectrum of speech. The size of the
    feature vectors represented by MFC method is 39 typically.

    In HMM-based ASR, the speech parameter vectors is a series of observations
    for training the HMM models. Each HMM model corresponds to one word.

    The acoustic model is a representation of the relationship between feature
    vectors (observations) and subphonemes (hidden states).
    Typically, each phoneme has three subphonemes. The Hidden Markov Model with
    Gaussian Mixture Models are used as the acoustic model in HMM-based ASR.

    The HMMs for different words are trained and we compare the model likelihoods
    by forward-backward algorithm for isolated words recognition. For continuous
    speech recognition, we join our word models with the silent state and use viterbi
    algorithm to decode words from speech.

    The language model is to provide the information about the priori probability
    of the word sequence $P(word)$. A language model is able to disambiguate between
    similar acoustics when combining linguistic knowledge and acoustic evidence.
    % For example, "ice cream" and "I scream" can be disambiguated with the language
    % model even though their pronunciation are the same.

    In HMM-based ASR, n-gram model is used as the language model and it is trained
    on millions of words of text.

    % an N-best list
    The N-Best list is a list contains N ranked possible decoding for the speech
    from the user.

    The reason why developers would like to get an N-best list as an ASR result
    is the top hypothesis may be erroneous and may not fit the content of a
    dialogue system and the correct hypothesis is often in the N-best list.
    Therefore, the N-best list can increase the robusness of a dialogue system.
\end{problem}


\begin{problem}{1.2}
    For the speech parametrization, MFCC is based on human hearing perceptions or
    in other words is to mimic known variation of the critical bands of
    human ears to different frequencies. Human ears are more discriminative at
    lower pitch and less discriminative at higher pitch and therefore the mel
    filterbank varies linearly below 1kHz and logarithmically above 1kHz.

    For the acoustic model, the hidden markov model considers the transition
    probability of a phoneme (state) to a phoneme or staying on one phoneme.
    It is in the same way as a proficient language user that understanding
    what a particular speech should sound like in relation to adjoining sounds.
    Humans would probabay understand a speech as a series of phonemes instead of
    phonemes independently.

    The language models gives the priori probability of a certain word in a
    particular context given the adjoining words. It is similar to how human
    guess a homophone word since human can guess the word by the converstation
    content and the regularity of the word based on the knowledge and understanding
    of a certain language.

    Since human and machine make error, the best guess of a word or a sentence may
    be erroneous. Human would perpare a list of guess of a speech and choose the
    best fit word or sentence given the converstation content and the intention
    of the speaker. The n-best lists can be considered as an imitation of this behavior.
\end{problem}

\begin{problem}{1.3}
    TODO
\end{problem}

\begin{problem}{2.1}
    In formant synthesis, speech output is parameterized by formant frequencies,
    fundamental frequency, their amplitudes, and noise levels etc. as a function
    of time. Formant speech synthesis is based on rules which describe
    the resonant frequencies of the vocal tract.

    In unit selection synthesis, the output speech is synthesized by selecting
    appropriate speech units from large databases of recorded speech.
    The speech units can be phone, diphone, half-phones or words.

    In diphone synthesis, speech is generated from a database which contains
    every diphone within the language.
    Rather using a large database including all speech units in unit selection
    synthesis, diphone synthesis only uses diphone as its basic unit.

    In HMM-based synthesis, the frequency spectrum,
    fundamental frequency (F0), and duration of speech
    are modeled simultaneously by HMMs.
    Speech are created based on the maximum likelihood estimation of state transitions.

    HMM-based synthesis is not concatenating different recordings
    as unit selection synthesis and diphone synthesis but generating speech parameters
    from the HMM models. And then a speech is synthesized from the generated parameters.

    During the systhesis phrase, unit selection synthesis is serial while HMM-based
    systhesis is parallel.
\end{problem}

\begin{problem}{2.2}

    % co-articulation yes, as in diphone
    Co-articulation means speech sounds tend to be influenced by other speech
    sounds surronding them.
    Diphone synthesis is able to capture this transition from one phonic unit
    to another unit.
    Therefore, unit selection synthesis is able to do the same thing that as
    diphone synthesis.

    The co-articulation is perfectly retained in the diphone or larger
    units.
    Therefore the unit selection synthesis can completely reproduce
    the co-articulation in synthesized speechs.


    % speech reduction
    Speech reduction means a speaker skips or changes some syllables when talking
    at normal speed. Unit selection synthesis can do the speech reduction at
    some degree and it depends on the speech data in database and how to annotate
    the text with phonetic units.
    For example, "Jag vet inte" in written Swedish is spoken as "javende". If
    the database have this phrase, then the synthesis can correct pronounce it
    correctly. On the other hand, if the database does not have this phrase,
    we have to annotate phonetic units correctly. However, the way to annotate
    phonetic units is case-by-case and it also depends on the context of the
    conversation.

    For human speakers, it is a natual action for native speakers and it depends
    on the knowledge to that language of the speakers.

    % hesitation
    For classical speech systhesis, it is hard to find any hesitations like
    a pause or an "ehm" since it is from text to speech.
    If we want to add hesitations in unit
    selection synthesis, we can add an extra module to analyze which part of the
    sentence would easily have a pause or an "emmmm" and then add hesitations with
    a low frequency.

    To compare with human speakers, the hesitations in the speech systhesis system
    may not be very natural since the analysis module can go wrong. Another point
    is hesitations is closely related to the conversation context and therefore
    human listeners may not feel natual to the hesitation.
    % using a random frequency to add a pause may not be very natual to humans.

    % word repetition
    In speech systhesis, word repetition would not happen since the systhesis
    system "translates" text to speech programmatically. Also, the speech data in
    the database is from a group of professional speakers and the data has quality control.
    Therefore it is hard to find word repetition in unit selection synthesis.

    To add word repetition, we can add a module to consider where should have
    word repetition such as the main topic of the sentence or the difficult words
    in the sentence and then create word repetition in the systhesised speech according
    to the analysis result.

    In unit selection synthesis, the word repetition is expected as a rare event
    since the synthesized speech sounds like from a professional speaker and therefore
    the word repetition phenomenon should not be very frequent.
\end{problem}

\begin{problem}{2.3}
    TODO
\end{problem}

% Question 3
\begin{problem}{3.1}
    TODO
\end{problem}

\begin{problem}{3.2}
    TODO
\end{problem}

% Question 4
\begin{problem}{4.1}
    Steps to make a data collection
    \begin{enumerate}
        \item Consider the purpose of the collected data. The purpose may be
        studying some linguistic behavior or making a speech recognition system.
        \item Design or determine data collection environment, equipments and technology.
        E.g. slient lab, telephone, or meeting room environment.
        What equipments would be used? What audio format should be used?
        \item Participant recruitment.
        \item Collect data of participant: name, gender, age, education, country born or raised in.
        \item Select participants for background varaible control and/or ecological validity
        \item Determine the best date and time for collecting data.
        \item Detemine in what form to collect data.
        E.g. reading text or an emulated situation with given topics.
        \item Data analyze, quality checking, and/or audio data to text transcription
    \end{enumerate}
\end{problem}

\begin{problem}{4.2}
    TODO
\end{problem}
% Question 5
\begin{problem}{5.1}
    Important steps of the process of evaluation:
    \begin{enumerate}
        \item Determine the target or purpose of the evaluation. This affects what kind
        of tests we need to do. For example, if we want to compare two
        speech synthesis systems, the grading test and the preference test will
        be helpful.

        \item User studies for representativeness. This is for controling the
        background varaibles in order to have a more representative evaluation
        result.

        \item Identify, select, and implement evaluation methods.
        It is mainly based on our purpose of the evaluation. For example, if the
        purpose of evaluation is to compare the new system and the existing system,
        grading tests and preference tests should be selected as the evaluation
        tests. For the implementation, it is down to details like design the questionaire
        for the grading tests in order to have a good construct validity.

        \item  Analyze and interpret results.
        There are multiple tasks related to this stage. Take a few tasks as examples.
        Calculating scores in the tests indicates the effectiveness of the speech system.
        Summerizing the test results and identifying the reason why A system is
        better than B system are the tasks should be done in this phrase. Moreover,
        the third party spectator can review the videos of the test period and it
        gives another point of view of interpreting the evaluation results.

    \end{enumerate}
\end{problem}
\pagebreak
\begin{problem}{5.2}
    Speech Recognition system and speech Synthesis system are chosen for this
    discussion.

    For a research instiution, the target of evaluation is to test hypotheses.

    In speech recognition, "good" means a modification perform better at some situation.
    For example, they change the feature extraction scheme from MFCCs to a new method
    and test if the accuracy of the new method is better than the MFCCs method.
    The data they collect is the number of words in the test utterance and
    the number words correctly recognized.

    In speech systhesis, "good" can mean how user understand the synthesized speech
    of a new speech systhesis system.
    If the user can understand the context given by the synthesized speech, the
    new speech systhesis system can be considered as good.
    The data like the score of a listening comprehension test should be collected
    for this purpose.

    To a R\&D company selling the underlying technology, the evaluation
    target could be how the new technology better than the existing one. The 
    evaluation can aslo be done on the current version and the old version of 
    the application in order to see the progress of development.

    In speech recognition, the company should compare accuracy across different 
    recognition technologies. Moreover, the performance and the stability of different 
    speech recognition technologies should be evaluated. The data like process time,
    accuracy and performance across a population of speakers should be collected.

    In speech systhesis, the data like intelligibility, smoothness, speed, and 
    fluency of the syntheized speech to human listeners should be collected. 
    Also, the performace and the computational resources like storage, computational
    power needed for each technology should be evaluated.
    The comparsion between two or more technologies on these asspects should be 
    made and see how good the new technology is.

    To a company selling the service to end users, the evaluation targets are
    the completeness of the service and the cost of the services.
    The evaluation for the completeness of the service is similar to the acceptance
    testing. The evaluation of cost services can be obtained by checking hardware
    cost, maintenance cost etc.

    In speech recognition, the data like a list of expected functionalities that 
    the application is able to acheive should be collected. In speech systhesis,
    the completeness of expected functionalities acheived
    should be collected. For example, we should check if the application is able
    to produce a minimal pairs that skilled listeners can distinguish.

    To an end user, the evaluation could be on the user experience. 
    The user experience questionaire should be used and then ask some general
    questions since the user experience should be user-oriented and we cannot
    ask very detailed questions. If the general feedback of the application is 
    positive comparatively, then the application can be considered as good.

    The questionaire should be designed very carefully and the different applications
    should have a slightly different sets of questions. 
    For speech recognition, one of the example questions would be "Do you satisfy
    with the speech recognition for completing your task?"
    For speech systhesis, it would be "do you understand the context or meaning
    of the synthesized speech?" 
    Moreover, third party spectators would be needed in order to have a more objective
    evaluation results for the user experience.
\end{problem}

Lorem ipsum dolor sit  ~\citep{King2007}.

\bibliographystyle{IEEEtranN}
\bibliography{ref}
\end{document}
