\documentclass[12pt]{article}

% packages
\usepackage[margin=1in]{geometry}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{subfig}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{cleveref}
\usepackage{listings}
\usepackage[makeroom]{cancel}
\usepackage[toc,page]{appendix}
\usepackage{enumerate}
\usepackage{csvsimple}
\usepackage{longtable}
\usepackage{pgfplotstable}
\usepackage{booktabs}


\setlength{\parindent}{1em}
\setlength{\parskip}{.3em}

% \newcommand{\N}{\mathbb{N}}
% \newcommand{\Z}{\mathbb{Z}}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
% \newcommand*\descriptionlabel[1]{\hspace\leftmargin$#1$}
\def\printemptyline#1{\def\par{\ifvmode\emptyline\fi\endgraf}\obeylines}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\begin{document}

\title{DT2112 home exam spring 2019}
\author{Lin Chun Hung, chlin3@kth.se}
\maketitle

\section{Automatic speech recognition and human perception}
\begin{problem}{1.1}
    A speech parametrization is for extracting the feature vectors or 
    the speech parameter vectors from speech signal. In the speech parametrization,
    the mel-frequency cepstrum coefficients (MFCCs) are most likely used for
    representing a short term power spectrum of speech. The size of the 
    feature vectors represented by MFC method is 39 typically. 

    In HMM-based ASR, the speech parameter vectors is a series of observations
    for training the HMM models. Each HMM model corresponds to one word.
    
    The acoustic model is a representation of the relationship between feature 
    vectors (observations) and subphonemes (hidden states).
    Typically, each phoneme has three subphonemes. The Hidden Markov Model with 
    Gaussian Mixture Models are used as the acoustic model in HMM-based ASR.

    The HMMs for different words are trained and we compare the model likelihoods
    by forward-backward algorithm for isolated words recognition. For continuous
    speech recognition, we join our word models with the silent state and use viterbi 
    algorithm to decode words from speech.

    The language model is to provide the information about the prior probability
    of the word sequence $P(word)$. A language model is able to disambiguate between
    similar acoustics when combining linguistic knowledge and acoustic evidence.
    % For example, "ice cream" and "I scream" can be disambiguated with the language
    % model even though their pronunciation are the same.
    
    In HMM-based ASR, n-gram model is used as the language model and it is trained
    on millions of words of text.
    
    % an N-best list 
    The N-Best list is a list contains N ranked possible decoding for the speech
    from the user.

    The reason why developers would like to get an N-best list as an ASR result
    is the top hypothesis may be erroneous and may not fit the content of a 
    dialogue system and the correct hypothesis is often in the N-best list. 
    Therefore, the N-best list can increase the robusness of a dialogue system.
\end{problem}


\begin{problem}{1.2}
    For the speech parametrization, MFCC is based on human hearing perceptions or
    in other words is to mimic known variation of the critical bands of 
    human ears to different frequencies. Human ears are more discriminative at 
    lower pitch and less discriminative at higher pitch and therefore the mel
    filterbank varies linearly below 1kHz and logarithmically above 1kHz.
    
    For the acoustic models, ...
    For the language models, ...
    For the N-best list, ...
\end{problem}

\begin{problem}{1.3}
    The parametrization is ...

    The acoustic models are...
\end{problem}


\end{document}
