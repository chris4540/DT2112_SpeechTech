\documentclass[12pt]{article}

% packages
\usepackage[margin=1in]{geometry}
\usepackage{enumerate}
\usepackage[numbers]{natbib}
\usepackage[hidelinks]{hyperref}
\usepackage{csvsimple}

% \setlength{\parindent}{1em}
\setlength{\parskip}{.5em}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\def\printemptyline#1{\def\par{\ifvmode\emptyline\fi\endgraf}\obeylines}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\begin{document}

\title{DT2112 Home Exam Spring 2019}
\author{Lin Chun Hung, chlin3@kth.se}
\maketitle

\section{Automatic speech recognition and human perception}
\begin{problem}{1.1}
    % parametrization
    A speech parametrization is for extracting the feature vectors or the speech
    parameter vectors from the speech signal. 
    In the speech parametrization,
    the Mel-frequency cepstrum coefficients (MFCCs) are most likely used for 
    representing a short term power spectrum of speech. 
    The size of the feature vectors represented by the MFCC method is 39 typically.
    In HMM-based ASR, the speech parameter vectors is a series of observations 
    for training the HMM models. Each HMM model 
    corresponds to one word.

    % acoustic models
    The acoustic model is a representation of the relationship between feature 
    vectors (observations) and sub-phonemes (hidden states).
    Typically, each phoneme has three sub-phonemes. 
    The Hidden Markov Model with the Gaussian Mixture Models is used as 
    the acoustic model in HMM-based ASR. 
    The HMMs for different words are trained and we compare the model likelihoods
    by the forward-backward algorithm for isolated words recognition. 
    For continuous speech recognition, we join our word models with the silent 
    state and use the Viterbi algorithm to decode words from speech.

    % language models
    The language model is to provide information about the priori probability of
    the word sequence $P(word)$. 
    A language model is able to disambiguate between similar acoustics when 
    combining linguistic knowledge and acoustic evidence.
    In HMM-based ASR, n-gram model is used as the language model and 
    it is trained on millions of words of text.

    % an N-best list
    The N-Best list is a list contains N ranked possible decoding for the speech
    from the user.
    The reason why developers would like to get an N-best list as an ASR result
    is the top hypothesis may be erroneous and may not fit the content of a
    dialogue system and the correct hypothesis is often in the N-best list.
    Therefore, the N-best list can increase the robustness of a dialogue system.
\end{problem}


\begin{problem}{1.2}
    % parametrization
    For the speech parametrization, MFCC is based on human hearing perceptions 
    or in other words is to mimic known variation of the critical bands of human 
    ears to different frequencies. 
    Human ears are more discriminative at a lower pitch and less discriminative 
    at a higher pitch and therefore the Mel filterbank varies linearly below 1kHz
    and logarithmically above 1kHz.

    % acoustic models
    For the acoustic model, the hidden Markov model considers the transition 
    probability of a phoneme (state) to a phoneme or staying on one phoneme. 
    It is in the same way as a proficient language user that understanding what
    a particular speech should sound like in relation to adjoining sounds. 
    Humans would probably understand a speech as a series of phonemes instead of 
    phonemes independently. Moreover, the topology of the hidden Markov model 
    for ASR allows retaining a state. 
    It means that ”Speech” and ”Speeeech”, which humans consider equivalent, 
    can fit the same word model well. 

    % language model
    The language model gives the priori probability of a certain word in a 
    particular context given the adjoining words. 
    It is similar to how human guess a homophone word since humans guess 
    the word by the conversation content and the regularity of the word based on
    the knowledge and understanding of a certain language. 
    
    % N-list
    Since human and machine make errors, the best guess of a word or a sentence 
    may be erroneous. 
    Humans would prepare a list of guess of a speech in their mind and choose 
    the best-fit word or sentence given the conversation context and the intention 
    of the speaker.
    The N-best lists can be considered as an imitation of this behavior.
\end{problem}

\begin{problem}{1.3}
    % https://www.youtube.com/watch?v=p5RWzBRg6rU
    % 1. The pitch of the speech
    % * signify questions
    % * emphasize the main point of the speech
    The first way can affect our perception is the pitch of the speech. 
    In English, the pitch can indicate whether this word is a verb or a noun. 
    The pitch can indicate the main point of a sentence and signify it is a question. 
    For example, ”You stole the book” with a high pitch at the end is a kind of
    question expecting an answer. 
    Humans can easily understand this is a question but an automatic speech 
    recognition (ASR) system would only translate it to a sentence if no 
    grammatical cues. 
    
    % http://www.speech.kth.se/~rolf/gslt_papers/MustaphaSkiri.pdf
    % 2. The visual event.
    % * Eye gazing refer to physical thing and fit to the speech context.
    % * McGurk effect 
    The second way is the visual event or the visual cue. 
    Our eye gaze is a strong indicator and it can give information to the speech context. 
    A simple example is that one speaker says ”Close the door please.” and 
    his/her gaze would give the location information of the door. 
    Another example is a girl say ”The room is very cold.” and she gazes at the windows. 
    Humans can understand that the girl is asking if the listener can close the windows. 
    Humans can understand these visual cues but an ASR would not understand the 
    context from the visual cues and just transcribe the speech to text.

    Another example of the visual event affecting speech perception is the McGurk effect. 
    This effect can be found when the speech sound does not match to the lip movement. 
    For example, the audio is ”ba” but the lip movement is ”fa” and the McGurk effect 
    takes place. 
    Humans understand ”fa” sound in the last example but an ASR system understands
    it as ”ba” sound since the system is just able to receive sounds.
\end{problem}

\pagebreak
\section{Synthesis and production}
\begin{problem}{2.1}
    In formant synthesis, speech output is parameterized by formant frequencies, 
    fundamental frequency, their amplitudes, and noise levels etc. as a function of time. 
    Formant speech synthesis is based on rules which describe the resonant frequencies 
    of the vocal tract. 
    
    In unit selection synthesis, the output speech is synthesized by selecting 
    appropriate speech units from large databases of recorded speech. 
    The speech units can be phone, diphone, half-phones or words. 
    
    In diphone synthesis, speech is generated from a database which contains every 
    diphone within the language. 
    Rather using a large database including all speech units in unit selection 
    synthesis, diphone synthesis only uses diphone as its basic unit. 
    
    In HMM-based synthesis, the frequency spectrum, fundamental frequency (F0), 
    and duration of speech are modeled simultaneously by HMMs. 
    Speech is created based on the maximum likelihood estimation of state transitions. 
    HMM-based synthesis is not concatenating different recordings 
    as unit selection synthesis and diphone synthesis but it generates speech parameters 
    from the HMM models. 
    Then a speech is synthesized from the generated parameters. 
    
    During the synthesis phase, unit selection synthesis is serial while 
    HMM-based synthesis is in parallel.
\end{problem}

\begin{problem}{2.2}

    % co-articulation yes, as in diphone
    Co-articulation means speech sounds tend to be influenced by other speech 
    sounds surrounding them. 
    Diphone synthesis is able to capture this transition from one phonic unit 
    to another unit. 
    Therefore, unit selection synthesis is able to do the same thing that as 
    diphone synthesis. 
    The co-articulation is perfectly retained in a diphone or a larger units.
    Therefore the unit selection synthesis can completely reproduce the 
    co-articulation in the synthetic speeches.


    % speech reduction
    Speech reduction means a speaker skips or changes some syllables when talking
    at normal speed. 
    Unit selection synthesis can do the speech reduction at some degree and 
    it depends on the speech data in the database and how to annotate the text 
    with phonetic units. 
    For example, ”Jag vet inte” in written Swedish is spoken as ”javende”. 
    If the database has this phrase, then the synthesis can correctly pronounce 
    it correctly. 
    On the other hand, if the database does not have this phrase, we have to 
    annotate phonetic units correctly. 
    However, the way to annotate phonetic units is case-by-case and 
    it also depends on the context of the conversation. 
    Speech reduction is a natural action for native speakers
    For second language speakers, it depends on the knowledge of the speaker 
    on that language.

    % hesitation
    For typical speech synthesis systems, it is hard to find any hesitations 
    like a pause or an ”ehm” since it is from text to speech.
    If we want to add hesitations in unit selection synthesis, we can add an 
    extra module to analyze which part of the sentence would easily have a 
    pause or an ”emmmm” and then add hesitations with a low frequency. 
    To compare with human speakers, the hesitations in the speech synthesis 
    system may not be very natural since the analysis module can go wrong. 
    Another point is hesitations is closely related to the conversation context 
    and therefore human listeners may not feel natural to the hesitation.

    % word repetition
    In speech synthesis, word repetitions would not happen since the synthesis 
    system "translates" text to speech programmatically. 
    Also, the speech data in the database is from a group of professional speakers
    and the data has good quality control. 
    Therefore it is hard to find word repetition in unit selection synthesis. 
    To add word repetition, we can add a module to consider where we should have 
    word repetitions (e.g. in the difficult words) and then create word repetitions 
    in the synthesized speech according to the analysis result. 
    In unit selection synthesis, word repetitions are expected as rare events 
    since the synthesized speech sounds like from a professional speaker and 
    therefore the word repetition in the system should not be frequent.
\end{problem}

\begin{problem}{2.3}
    There are three ways to stop and resume the book reading system. 
    The system can stop immediately, can stop a few words later, or stop until 
    a clause or a segment of a sentence is completed when it received the stop command. 
    
    There are three corresponding ways to resume the system. 
    The system can restart from the point it stopped, from a few words before 
    the point it stopped, or from the beginning of the sentence where it stopped. 
    
    Three suggestions are given to simulate a human-like pause and resumption in
    a unit selection synthesis.

    The first suggestion is to consider the synthesized speech as an audio file.
    We pause and resume the synthetic speech as what we do in a media player
    application. We mark down the time to the audio file when we stop and resume
    it back using the recorded time.
    This method is good because it is easy to understand and implement it. 
    The pause is not quite human-like because we will not pause it like a playing machine. 
    
    
    The second suggestion is to online re-synthesize the ending phrase 
    (the few words after receiving the pause command) and the starting words 
    (the few words before the pause point). 
    The tone and the prosody will be adjusted in this situation and therefore 
    it is more human-like. 
    
    The third method needs to break a sentence into segments. 
    We first use a model to separate a sentence into segments and each segment 
    has a reasonable number of words, say like at most 10 words. 
    The model for breaking down the sentence can be a statistical model or a 
    rule-based model. 
    The speech synthesis system generates one speech to one segment instead of 
    the whole sentence.
    This is to emulate the human understanding structure of a sentence. 
    When we read a long sentence, we usually separate it into a few parts. 
    The separation rules may be from conjunction words, punctuation marks, 
    a clause, or the meaning of a group of words. 
    When we need to stop, we stop at the end of the current speaking segment 
    and restart it back from the last segment. It is a good idea as we try to mimic
    the human's behavior in the same situation.
\end{problem}

\pagebreak
% Question 3
\section{Dialogue}
\begin{problem}{3.1}
    % smart home manager
    % data checking
    % https://peterhurley.com/blog/2018/google-assistant-vs-human-assistant-featuring-peter-hurley
    I would like to use Google Home Assistant to mention 4 ways where a dialogue 
    system performs a well-defined dialogue task differs from a human to do so.

    % Physically
    First, a human assistant barely replaces you in the physical interaction or 
    operation but it is different to Google Home Assistant. 
    For example, I call Peter, my human assistant, to turn on the light of 
    the living room and then he goes to the switch of the light and turn it on. 
    On the other hand, the Google Home Assistant can help you turn on the light 
    through the home intranet without any physical operation. 
    
    % Information Obtaining time 
    Second, the time for obtaining information is different. 
    A spoken dialogue system will be much faster. 
    For instance, I ask Google Home Assistant what is the temperature today and 
    it will response me just one second. 
    A human assistant takes time to go to check the online website. 
    For another question like ”Any milk in the fridge?”, the same thing happens. 
    
    % Visual interation
    However, a human assistant performs better when the task needs visual cues, 
    visual interactions, my position information, head pose, and/or eye gaze. 
    For example, I say "Close the door please." and the human assistant could 
    understand that I want to close the door in the room I am currently staying in. 
    However, the Google assistant may ask "Which door?" or say 
    "I don’t understand your order. Could you say one more time?".
    
    % Robustness with understanding error.
    Lastly, a human assistant has high robustness with the understanding error. 
    When I say "Prepare the oven. Please." which means I would like to preheat 
    the oven, the Google assistant may be not understand my order as the system 
    does not expect it, whereas my human assistant is able to understand 
    my request and do it correctly.
\end{problem}

\begin{problem}{3.2}
    % https://botsociety.io/blog/2018/03/chatbot-examples/
    For the application fitting the ”interface replacement” view, the application
    called ”1800Flowers” allows users to order flowers and the delivery service.
    It is because a web form or an online flower shop website can have the same 
    functionality as this application.

    % https://www.facebook.com/DrCareAi/photos/a.2123527274339622.1073741828.2076696632356020/2316047921754222/?type=3
    For the application fit the ”human substitute” view, a chatbot with speech 
    support called”Dr. Care helper” from Hong Kong is for users to make an 
    appointment with a medical specialist, give medical advice, 
    check how full the public hospital is, and estimate how longa patient have 
    to wait for a specialist in the public hospital. 
    (Yes, Hong Kong has a serious medical service shortage in the public sector.) 
    This chatbot gathers all the related information and gives some meaningful 
    suggestions to users. 
    Therefore, the spoken dialogue system can be considered to be like a human helper.

    % https://sumo.com/stories/ecommerce-chatbot-marketing
    % https://www.chatbotguide.org/ebay
    Some spoken dialogue systems are less obvious to see which group they belong to. 
    For example, it is hard to classify the shopbot from eBay and Google Assistant. 
    
    The shopbot can help you to find an item with a vague request like 
    "I am looking for a jacket" and then suggest some items. 
    Also, you can buy the selected item and send it to your home. 
    Therefore, it is hard to classify because the application combines 
    the features from these two categories. 
    The good thing for the shopbot is it increases the user experience. 
    From introducing a product to make a payment, you can complete all the task
    within one chatbot. It makes the chatbot like a salesman. 
    The disadvantage of this chatbot is its complexity. 
    When the number of available tasks increases, the implementation difficulty,
    and the error rate increases. 
    When the chatbot recognized a speech wrong, it will easily get a wrong 
    conversation intention. It is because the number of intention is high for a complex 
    chatbot and the probability of matching the wrongly recognized speech is higher
    than a simple chatbot with few designed intents.

    The good thing for the shopbot is it increases the user experience. From 
    introducting product to make payment, you can complete all the task within 
    one chatbot. It makes the chatbot like a salesman. The disadvantage of this
    chatbot is its complexity. When the number of avaibable tasks increases, 
    the implementation difficulty and the error rate increases. When the chatbot
    recognized a speech wrong, it will easily get a wrong conversation intention
    since the number of intention is high for a complex chatbot.

    % pros and cons for google assistant
    Google assistant on smartphones is considered as a human substitute 
    (e.g, play games via speech, book a table, or sing the happy birthday song) 
    and an interface replacement (e.g. set an alarm or a timer via speech, 
    search something via speech, and set a reminder). 
    The advantage is the google assistant is more human-like and as a true 
    "virtual assistant" since the assistant is integrated with other Google 
    services. 
    The disadvantage is the user privacy is not really respected. 
    Your travel schedule, dialogue history, and even your voice would be sent 
    to Google for their analysis.
\end{problem}

\begin{problem}{3.3}
    The Google assistant on my phone is used as the subject for this task.
    Japanese is chosen as the language for the investigation.

\noindent\textbf{Describe the system}

    The Google assistant has at least 4 components: automatic speech recognizer 
    (ASR), dialog manager, response generation, and text-to-speech (TTS) synthesis. 
    First, the ASR component transforms the voice input from the user to text. 
    Second, the dialog manager interprets the intent of the recognized speech 
    text and then obtain the relevant information from other modules. 
    After that, the response generation component creates a text response and 
    the text-to-speech synthesis component reads it out.

    I asked three kinds of questions, the current weather, my current location, 
    and what could you (refer to the assistant) do. 
    Each question has two manners: normally (teineigo, polite language) and 
    respectfully (sonkeigo, respect language). 
    Since the assistant shows what it listened and what it will say, 
    the TTS synthesis component and the ASR component were then verified. 
    With a slight variated questions, the Google assistant did what I expected 
    like telling me back the current temperature, 
    a brief weather report (raining, snowing or cloudy), and telling me back 
    which Stockholm district I was in and the brief introduction of its functionality.

\noindent\textbf{Describe the interaction}

    The dialogue was like a ping-pong game, the assistant almost responded immediately.
    There is no restriction of the topics and I can ask whatever I want. 
    I have to press the "microphone" button before speaking. 
    The user can interrupt the conversation by pressing the "microphone" button.
    The system shows what it was recognized on the screen. 
    For the confirmation, the assistant asked ”<comfirmation context>, is it okay?” 
    to seek my confirmation.

\noindent\textbf{Describe the error management}

    When I said something wrong, there were two situations. 
    The first one was they had a matched intent and the assistant brought 
    the conversation to the matched intent. 
    The second one is the assistant said that it did not understand what I said 
    and asked me to say one more time. 
    It is easy to correct misunderstandings and I just need to say it again.
    If I have multiple errors, I will need to start over the conversation again.
    For example, I asked the assistant to set an alarm and I failed to let the 
    assistant understand what I said. 
    The assistant dropped the current conversation and I had to do it again.

\noindent\textbf{Discuss the user friendliness the system}

    The system is easy to use. 
    I can say anything and it allows a great variant of my questions. 
    The system is efficient and I do not need to wait for its response. 
    I like to use it as it is multi-functional and it is a good replacement 
    for the physical user interface. 
    I do not need to press on the screen multiple times to do some simple tasks.

\end{problem}

\pagebreak
% Question 4
\section{Data collection}
\begin{problem}{4.1}
    Steps to make a data collection
    \begin{enumerate}
        \item Consider the purpose of the collected data. The purpose may be
        studying some linguistic behavior or making a speech recognition system.
        \item Design or determine the data collection environment, equipments and technology.
        E.g. slient lab, telephone, or meeting room environment.
        What equipments would be used? What audio format should be used?
        \item Participant recruitment.
        \item Collect participant's data: name, gender, age, education, country born or raised in.
        \item Select participants for background varaible control and/or ecological validity
        \item Determine the best date and time for collecting data.
        \item Detemine in what form to collect data.
        E.g. reading text or an emulated situation with given topics.
        \item Data analyze, quality checking, and/or transcribe audio data to text
    \end{enumerate}
\end{problem}

\begin{problem}{4.2}
    % Switchboard: https://catalog.ldc.upenn.edu/LDC97S62
    % Columbia Games Corpus: http://www.cs.columbia.edu/speech/games-corpus/
    % D64: http://www.uni-bielefeld.de/lili/personen/pwagner/papers/lrec10_oertel_etal.pdf
    \begin{table}
        \centering
        \csvautotabular[respect dollar=false]{csv/task_4_2_p1.csv}
        \caption{Basic properties of the corpora}
        \label{tbl:Q42_corpora_properties}    
    \end{table}

    \begin{table}
        \centering
        \csvautotabular[respect dollar=false]{csv/task_4_2_p2.csv}
        \caption{Characteristic of the corpora}
        \label{tbl:Q42_corpora_char}    
    \end{table}

    The basic properties of the corpora can be found in the Table \ref{tbl:Q42_corpora_properties}.
    The language used in D64 is unknown since there is no sample data and no 
    specification in the paper for D64 \cite{oertel2013d64}.

    The Characteristic of the corpora can be found in the Table \ref{tbl:Q42_corpora_char}.

\noindent\textbf{Purpose}

    The Switchboard is a general purpose corpus. It is suitable for building,
    testing, and evaluating an automatic speech recognition system targeting to 
    telephone conversations. \cite{switchboard_intro}

    The Columbia Games Corpus is originally for a study of the intonational 
    realization of given verse new information. 
    It can also be used in developing and evaluating an interactive voice 
    response (IVR) systems especially in the aspect of turn generation and recognition.
    \cite{moller2007evaluating}

    The D64 corpus is for studying naturalistic, ethologically situated, and 
    conversational interaction \cite{oertel2013d64}. 
    Also,  the motivation for building this corpus was to make the participants 
    focus on social interaction. 
    It reflects on posture, hand and arm gestures, and eye gaze \cite{oertel2013d64}.

\noindent\textbf{The highlighed characteristic}
    
    The most different characteristic of the switchboard corpus is its variety 
    and rich annotations. 
    The variety and the richness of annotations can make the evaluation of 
    a speech recognition system more representative. 

    The highlighted characteristic of the Columbia Games Corpus is its 
    co-operation task-oriented dialogue. 
    Each speech involves the coordination of speakers and turns exchanges 
    between them. 
    In evaluating a speech recognition system for multiple users, 
    this corpus may be helpful.

    For D64 corpus, the rich recording will be the highlighted point.
    It would be helpful for those visual speech systems.
\end{problem}

\begin{problem}{4.3}
    % sub-question 1
    For a corpus for training acoustic models for speech recognition of street 
    names in an in-car environment, three features are important. 
    They are control over the environment, age balance, and gender balance.

    In a car cabin, it could be quiet in as the car is new or noisy as it has an
    old engine.
    Opening a car window will greatly raise the noise level of the cabin. 
    Therefore, we should have good control over the environment and select 
    multiple noise levels for emulating the real-world situation. 
    The driver voices would have a great variety since anybody over 18 years old 
    could obtain a driving license. 
    Hence the age and gender of speakers should be balanced.

    % sub-question 2
    For a corpus for broad vocabulary unit selection synthesis of academic literature, 
    audio quality, control over the task performed, and control over the linguistic 
    background of subjects are the most important features.  
    Ideally more professional academic materials can lower the target cost and 
    the join cost of the system. 
    Therefore, control over the task performed is important. 
    The recording quality and the background of the speaker are important 
    since it is for a speech synthesis system.
   
    % sub-question 3
    For a corpus of a spoken dialogue system which guides visitors to a museum 
    for studying semantic concepts, control over the tasks in the dialogue, 
    recordings of a large number of subjects, and the mobility of the subjects 
    are important. 
    In that dialogue system, pointing directions or other instructions need to 
    be well designed and therefore the control over the tasks is important.
    Since it is a guiding dialogue system, the subjects should be able to move 
    around. 
    Semantic concepts should be converged in a large population and 
    therefore we need a large number of subjects. 

    % sub-question 4
    A corpus for scientific studies of breath, posture, and speech for turn-taking
    in human face-to-face interaction should focus on if it is an ecologically 
    valid task, synchronization of different recordings and extensive 
    recordings of each subject.
    Since the corpus is for studying human interactions, a normal circumstance 
    would be a requirement. 
    A laboratory environment may not be a good idea.
    Also, the visual speech relation is important in this task and therefore
    the time alignment between video recordings and audio recordings is important. 
    To capture the detailed interactions in a dialogue, extensive recordings are
    necessary for breath, heartbeat rate etc.
\end{problem}
\pagebreak

% Question 5
\section{Evaluation}
\begin{problem}{5.1}
    Important steps of the process of evaluation:
    \begin{enumerate}
        \item Determine the target or purpose of the evaluation. This affects what kind
        of tests we need to do. For example, if we want to compare two
        speech synthesis systems, the grading test and the preference test will
        be helpful.

        \item User studies for representativeness. This is for controling the
        background varaibles to have a more representative evaluation
        result.

        \item Identify, select, and implement evaluation methods.
        It is mainly based on our purpose of the evaluation. For example, if the
        purpose of evaluation is to compare the new system and the existing system,
        grading tests and preference tests should be selected as the evaluation
        tests. For the implementation, it is down to details like designomg
        the questionnaire for the grading tests for a good construct validity.

        \item  Analyze and interpret results.
        There are multiple tasks related to this stage. Take a few tasks as examples.
        Calculating scores indicates the effectiveness of the speech system.
        Summerizing the test results and identifying the reason why A system is
        better than B system are the tasks should be done in this phrase. Moreover,
        the third party spectator can review the videos of test takers in 
        the test period and it gives another point of view of interpreting 
        the evaluation results.

    \end{enumerate}
\end{problem}

\begin{problem}{5.2}
    Speech Recognition system and speech Synthesis system are chosen for this
    discussion.

    For a research instiution, the target of evaluation is to test hypotheses.

    In speech recognition, "good" means a modification performs better at some situations.
    For example, they changed the feature extraction scheme from MFCCs to a new method
    and test if the accuracy of the new method is better than the MFCCs method.
    The data collected is the number of words in the test utterance and
    the number words correctly recognized.

    In speech systhesis, "good" means how user understands the synthesized speech
    of a new speech systhesis system.
    If the user is able to understand the context given by the synthesized speech, the
    new speech systhesis system will be considered as good.
    The data like the score of a listening comprehension test should be collected
    for this purpose.

    To a R\&D company selling the underlying technology, the evaluation
    target could be how the new technology better than the existing one. The 
    evaluation can also be done between the current version and the old version of 
    the application in order to see the progress of the software development.

    In speech recognition, the company should compare accuracy across different 
    recognition technologies. Moreover, the performance and the stability of different 
    speech recognition technologies should be evaluated. The data like process time,
    accuracy and performance across a population of speakers should be collected.

    In speech systhesis, the data like intelligibility, smoothness, speed, and 
    fluency of the syntheized speech to human listeners should be collected. 
    Also, the performace and the computational resources like storage, computational
    power needed for each technology should be evaluated.
    The comparsion between two or more technologies on these asspects should be 
    made and see how good the new technology is.

    To a company selling the service to end users, the evaluation targets are 
    the completeness of the service and the cost of the services. 
    The evaluation for the completeness of the service is similar to the 
    acceptance testing. The evaluation of cost services can be obtained by 
    checking hardware cost, maintenance cost etc.

    In speech recognition, a list of expected functionalities should be collected. 
    In speech synthesis, the completeness of expected functionalities achieved 
    should be collected. For example, we should check if the application is 
    able to produce minimal pairs that skilled listeners can distinguish.

    To an end user, the evaluation could be on the user experience. 
    The user experience questionnaire should be used and then ask some general 
    questions since the user experience should be user-oriented and we cannot 
    ask very detailed questions. If the general feedback of the application is
    positive comparatively, then the application can be considered as good. 

    The questionnaire should be designed very carefully and the different 
    applications should have a slightly different sets of questions. 
    For speech recognition, one of the example questions would be 
    "Do you satisfy with the speech recognition for completing your task?" 
    For speech synthesis, it would be "Do you understand the context or meaning of the synthesized speech?"
    Moreover, third-party spectators would be needed for a more objective 
    evaluation result on the user experience.
\end{problem}

\begin{problem}{5.3}
    Three academic research evaluations are selected. 
    
\noindent\textbf{Motivations}

    The first evaluation method is for evaluating the performance of a speech 
    recognition method in a noisy condition. \cite{hirsch2000aurora}
    
    The second and the third evaluation method are for a HMM-based speech 
    synthesis system in the Thai language \cite{chomphan2007implementation}.    
    The second evaluation method is to present the correctness of the synthesized 
    tone since the Thai language is a tonal language and has five tones.
    The third method is to evaluate the synthesized speech quality.

\noindent\textbf{Methods}

    In the first evaluation method, they first prepared a speech data database 
    for training and testing the speech recognition system. 
    The speeches in the database were first obtained from the corpus called 
    TIdigits and then add 8 different real-world noises to the speech with 
    different noise ratios \cite{hirsch2000aurora}.
    After training the speech recognition system, the system tested with different
    conditions test set and evaluate the performance based on word accuracy.

    In the second method, they asked listeners to evaluate the correctness of the 
    tone in a synthesized speeches syllable-by-syllable. \cite{tone_questions} 
    
    In the third task, they asked listeners to give the mean opinion score to 
    the naturalness of synthesized speeches. After that, they asked listeners
    to compare the naturalness of synthesized speeches from the HMM-based system and 
    a unit-selection-based TTS system and then give the relative scores.

\noindent\textbf{Human judge involved or automatic measures}
    
    The first evaluate method is objective and could be done automatically since
    all test data are from the corpus which means it should be labeled.

    The second and the third evaluation method are subjective and involve human judges.
    I would say that the second evaluation method is less subjective since 
    distinguishing different pitches of speeches should be a common standard to
    native speakers.

\noindent\textbf{Critique}

    In the first evaluation method, using the speech data beyond digits would 
    be more representative since the corpus TIdigits has only speeches of
    connected digits in American English. However, the cost of collecting speeches
    will be high if they cannot find an existing corpus.

    In the second method, a naive algorithm for compare tones should be used in 
    order to have an objective result. For example, in a pair of speeches, 
    one is synthetic and one is natural, align and normalize the syllable 
    and calculate the moving average of the F0 frequency of the pair of the syllable.
    After that, compare their moving average frequency numerically.

    In the third method, the questionnaire that we can find online mentions
    that the naturalness should be concerned. Adding more questions for different
    aspects like understandable and speed may help to improve this evaluation.
\end{problem}
\pagebreak

\bibliographystyle{IEEEtranN}
\bibliography{ref}
\end{document}
