\documentclass[12pt]{article}

% packages
\usepackage[margin=1in]{geometry}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{subfig}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{cleveref}
\usepackage{listings}
\usepackage[makeroom]{cancel}
\usepackage[toc,page]{appendix}
\usepackage{enumerate}
\usepackage{csvsimple}
\usepackage{longtable}
\usepackage{pgfplotstable}
\usepackage{booktabs}


\setlength{\parindent}{1em}
\setlength{\parskip}{.3em}

% \newcommand{\N}{\mathbb{N}}
% \newcommand{\Z}{\mathbb{Z}}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
% \newcommand*\descriptionlabel[1]{\hspace\leftmargin$#1$}
\def\printemptyline#1{\def\par{\ifvmode\emptyline\fi\endgraf}\obeylines}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\begin{document}

\title{DT2112 home exam spring 2019}
\author{Lin Chun Hung, chlin3@kth.se}
\maketitle

\section{Automatic speech recognition and human perception}
\begin{problem}{1.1}
    A speech parametrization is for extracting the feature vectors or 
    the speech parameter vectors from speech signal. In the speech parametrization,
    the mel-frequency cepstrum coefficients (MFCCs) are most likely used for
    representing a short term power spectrum of speech. The size of the 
    feature vectors represented by MFC method is 39 typically. 

    In HMM-based ASR, the speech parameter vectors is a series of observations
    for training the HMM models. Each HMM model corresponds to one word.
    
    The acoustic model is a representation of the relationship between feature 
    vectors (observations) and subphonemes (hidden states).
    Typically, each phoneme has three subphonemes. The Hidden Markov Model with 
    Gaussian Mixture Models are used as the acoustic model in HMM-based ASR.

    The HMMs for different words are trained and we compare the model likelihoods
    by forward-backward algorithm for isolated words recognition. For continuous
    speech recognition, we join our word models with the silent state and use viterbi 
    algorithm to decode words from speech.

    The language model is to provide the information about the prior probability
    of the word sequence $P(word)$. A language model is able to disambiguate between
    similar acoustics when combining linguistic knowledge and acoustic evidence.
    % For example, "ice cream" and "I scream" can be disambiguated with the language
    % model even though their pronunciation are the same.
    
    In HMM-based ASR, n-gram model is used as the language model and it is trained
    on millions of words of text.
    % an N-best list 
\end{problem}


\begin{problem}{1.2}
    The parametrization is ...
    
    The acoustic models are...
\end{problem}

\begin{problem}{1.3}
    The parametrization is ...

    The acoustic models are...
\end{problem}


\end{document}
